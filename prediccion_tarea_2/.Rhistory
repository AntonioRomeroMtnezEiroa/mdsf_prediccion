knitr::opts_chunk$set(echo = TRUE)
nba <- read_csv("nba.csv")
install.packages("tidyverse")
library(tidyverse)
nba <- read_csv("nba.csv")
View(nba)
install.packages("janitor")
knitr::opts_chunk$set(echo = TRUE)
library(here) # Comentar
install.packages("here")
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
install.packages("skimr")
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
install.packages("corrplot")
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
install.packages("ggcorrplot")
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
install.packages("PerformanceAnalytics")
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
install.packages("leaps")
knitr::opts_chunk$set(echo = TRUE)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
raw_data <-  read.csv("nba.csv")
raw_data <-  read.csv("C:\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\nba.csv")
raw_data <-  read.csv("\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\nba.csv")
raw_data <-  read.csv("\C:\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\na.csv")
raw_data <-  read.csv("\C\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\na.csv")
raw_data <-  read.csv("\C:\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\nba.csv")
raw_data <-  read.csv("C:\Users\arome\OneDrive\Escritorio\cp_01_nba\prediccion_tarea_1\nba.csv")
raw_data <-  read.csv("nba.csv")
colnames(raw_data)
raw_data %<>% clean_names()
colnames(raw_data)
skim(raw_data)
# delete duplicate
# Remove duplicate rows of the dataframe
raw_data %<>% distinct(player,.keep_all= TRUE)
# delete NA's
raw_data %<>% drop_na()
# Summarise
skim(raw_data)
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(magrittr)
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(rsample)
library(glmnet)
nba <- read_csv("nba.csv")
nba
nba %<>% clean_names()
colnames(nba)
skim(raw_data)
skim(nba)
nba %<>% distinct(player, .keep_all = T)
summarise_all(nba, funs(sum(is.na(.))))
nba %<>% drop_na()
skim(nba)
nba %>%
select_at(vars(-c("player","nba_country","tm"))) %>%
tidyr::gather("id", "value", 2:25) %>%
ggplot(., aes(y=salary, x=value))+
geom_point()+
geom_smooth(method = "lm", se=FALSE, color="black")+
facet_wrap(~id,ncol=2,scales="free_x")
nba %>%
select_at(vars(-c("player","nba_country","tm"))) %>%
tidyr::gather("id", "value", 2:25) %>%
ggplot(., aes(y=log(salary), x=value))+
geom_point()+
geom_smooth(method = "lm", se=FALSE, color="black")+
facet_wrap(~id,ncol=2,scales="free_x")
log_nba <- nba %>% mutate(salary=log(salary))
skim(log_nba)
# Excluded vars (factor)
vars <- c("player","nba_country","tm")
# Correlations
corrplot(cor(log_nba %>%
select_at(vars(-vars)),
use = "complete.obs"),
method = "circle",type = "upper")
# Other Correlations
ggcorrplot(cor(log_nba %>%
select_at(vars(-vars)),
use = "complete.obs"),
hc.order = TRUE,
type = "lower",  lab = TRUE)
# Other Correlations
chart.Correlation(log_nba %>%
select_at(vars(-vars)),
histogram=TRUE, pch=19)
model_vif <- lm(salary~.-player-nba_country-tm, data=log_data)
model_vif <- lm(salary~.-player-nba_country-tm, data=log_nba)
vif_values <- car::vif(model_vif)
library(car)
install.packages("car")
library(car)
model_vif <- lm(salary~.-player-nba_country-tm, data=log_nba)
vif_values <- car::vif(model_vif)
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue")
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
knitr::kable(vif_values)
set.seed(123)
nba_split <- initial_split(nba, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
set.seed(123)
nba_split <- initial_split(log_nba, prop = .7, strata = "salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- log(nba_train$salary)
nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$salary)
dim(nba_train_x)
nba_ridge <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
plot(nba_ridge, xvar = "lambda")
nba_ridge$lambda %>% head()
ames_ridge_cv <- cv.glmnet(
x = ames_train_x,
y = ames_train_y,
alpha = 0
)
nba_ridge_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 0
)
plot(nba_ridge_cv)
min(nba_ridge_cv$cvm)
nba_ridge_cv$lambda.min
log(nba_ridge_cv$lambda.min)
ames_nba_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]
nba_ridge_cv$lambda.1se
log(nba_ridge_cv$lambda.1se)
plot(ames_ridge, xvar = "lambda")
plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_ridge_cv, s = "lambda.1se") %>%
broom::tidy() %>%
filter(row != "(Intercept)") %>%
top_n(25, wt = abs(value)) %>%
ggplot(aes(value, reorder(row, value))) +
geom_point() +
ggtitle("Top 25 influential variables") +
xlab("Coefficient") +
ylab(NULL)
nba_lasso <- glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
plot(nba_lasso, xvar = "lambda")
# Apply CV Ridge regression to ames data
nba_lasso_cv <- cv.glmnet(
x = nba_train_x,
y = nba_train_y,
alpha = 1
)
# plot results
plot(nba_lasso_cv)
min(nba_lasso_cv$cvm)
nba_lasso_cv$lambda.min
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]
nba_lasso_cv$lambda.1se  # lambda for this MSE
plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")
coef(nba_lasso_cv, s = "lambda.1se") %>%
tidy() %>%
filter(row != "(Intercept)") %>%
ggplot(aes(value, reorder(row, value), color = value > 0)) +
geom_point(show.legend = FALSE) +
ggtitle("Influential variables") +
xlab("Coefficient") +
ylab(NULL)
# minimum Ridge MSE
min(nba_ridge_cv$cvm)
# minimum Lasso MSE
min(nba_lasso_cv$cvm)
knitr::opts_chunk$set(echo = TRUE)
library(here) # Comentar
library(tidyverse)
library(janitor) # Clean names
library(skimr) # Beautiful Summarize
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(MASS)
library(dplyr)
library(readr)
library(gvlma)
library(MASS)
library(car)
library(glmnet)
library(boot)
library(leaps)
library(rsample)
library(imputeTS)
suppressWarnings(expr)
df <-  read.csv("nba.csv")
colnames(df)
df %<>% clean_names() #"limpiamos" las columnas
colnames(df) #vemos si ha surgido efecto sobre los nombres de las columnas
df %<>% distinct(player,.keep_all = TRUE) # eliminamos jugadores repetidos como en la practica anterior
df <- na_mean(df, option = "median") # a pesar de haber pocos nulos los cambiamos por su media
skim(df) #analisis de estadisticos resumidos
log_df <- df %>% mutate(salary = log(salary)) #ponenmos de forma logaritmica el salario
#realizamos un grafico de correlaciones, sin las variables especificadas.
categoricas <- c("player","nba_country","tm")
ggcorrplot(cor(log_df %>%
select_at(vars(-categoricas)),
use = "complete.obs"),
hc.order = TRUE,
method = "square",
type = "lower",
ggtheme = ggplot2::theme_light,
colors = c("orange", "black", "blue"),
title = "Matriz de correlación de variables numéricas",
tl.srt = 90,
lab = FALSE)
set.seed(2020) # muestreo aleatoria
particion <- initial_split(log_df, prop = .8, strata = "salary") # Cogemos el 80% para hacer la fase de train
train <- training(particion) # dataset para training
test  <- testing(particion) #dataset para testing
train_x <- model.matrix(salary ~ . -player -nba_country -tm, data = log_df)[, -1]
train_y <- log_df$salary
test_x <- model.matrix(salary ~ . -player -nba_country -tm, data = log_df)[, -1]
test_y <- log_df$salary
#tabla con los errores de cada valor de alpha
muestreo <- sample(1:10, size = length(train_y), replace = TRUE)
# Miramos entre los grupos de alphas.
tabla_errores_alpha <- tibble::tibble(
alpha      = seq(0, 1, by = 0.04),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
tabla_errores_alpha
for (i in seq_along(tabla_errores_alpha$alpha)) {
alpha_min <- cv.glmnet(train_x, train_y, alpha = tabla_errores_alpha$alpha[i], foldid = muestreo)
# Se sacan los minimun square errors y lambdas
tabla_errores_alpha$mse_min[i]    <- alpha_min$cvm[alpha_min$lambda == alpha_min$lambda.min] #error minimo cuadrado
tabla_errores_alpha$mse_1se[i]    <- alpha_min$cvm[alpha_min$lambda == alpha_min$lambda.1se] #lambda que minimiza el error minimo cuadrado
tabla_errores_alpha$lambda_min[i] <- alpha_min$lambda.min #error minimo standard
tabla_errores_alpha$lambda_1se[i] <- alpha_min$lambda.1se #lambda que minimiza el error minimo standard
}
tabla_errores_alpha <- tabla_errores_alpha %>%
arrange(mse_min)
tabla_errores_alpha #donde vemos que el alpha que minimiza el mse_min es = 1 lo que nos indica wue es un Lasso
error_train   <- cv.glmnet(train_x, train_y, alpha = 1.0)
min(error_train$cvm)
pred <- predict(error_train, s = error_train$lambda.min, test_x)
mean((test_y - pred)^2) #el error de la prediccion es menor que
